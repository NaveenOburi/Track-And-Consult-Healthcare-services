{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment _1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNJ4GVsbN47VST78snK9sF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaveenOburi/Track-And-Consult-Healthcare-services/blob/master/Assignment__1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxzIaoxR4F5X",
        "colab_type": "code",
        "outputId": "4e8e8c61-b465-4226-9be8-2bc7a50cdca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb9HJfoIFaUj",
        "colab_type": "text"
      },
      "source": [
        "import the libraries to work with the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ1px1pa5l6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the pandas library to read our dataset\n",
        "import pandas as pd\n",
        "#Import the numpy library to work with and manipulate the data\n",
        "import numpy as np\n",
        "# Import the pytorch library\n",
        "import torch\n",
        "# Import the time\n",
        "import time\n",
        "# to show the plot \n",
        "import matplotlib.pyplot as plt\n",
        "# Get the train/test split package from sklearn for preparing our dataset to train and test the model with\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import the 1D convolution layer\n",
        "# Since we’re inputting a 1 dimensional row of data, we can’t use 2D or 3D\n",
        "from torch.nn import Conv1d\n",
        "#Import the max pooling layer\n",
        "from torch.nn import MaxPool1d\n",
        "# Import the flatten layer\n",
        "from torch.nn import Flatten\n",
        "# Import the linear layer\n",
        "from torch.nn import Linear\n",
        "# Import the Sigmoid activation function\n",
        "from torch.nn.functional import sigmoid\n",
        "# Import the DataLoader and TensorDataset libraries from PyTorch\n",
        "# to work with the datasets\n",
        "from torch.utils.data import DataLoader,TensorDataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B-8-nO5O_p0",
        "colab_type": "text"
      },
      "source": [
        "To read , clean the dataset alos print the first ten records of the housing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHiacnQA5s-B",
        "colab_type": "code",
        "outputId": "76bf73e1-5da1-4573-b856-e1df14a1f52d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "dataset=pd.read_csv('/content/housing.csv')\n",
        "dataset=dataset.dropna()\n",
        "# Print the first ten records of the dataset.\n",
        "dataset.head(10)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "      <th>ocean_proximity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.23</td>\n",
              "      <td>37.88</td>\n",
              "      <td>41.0</td>\n",
              "      <td>880.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>8.3252</td>\n",
              "      <td>452600.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-122.22</td>\n",
              "      <td>37.86</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7099.0</td>\n",
              "      <td>1106.0</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>1138.0</td>\n",
              "      <td>8.3014</td>\n",
              "      <td>358500.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-122.24</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1467.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>496.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>7.2574</td>\n",
              "      <td>352100.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1274.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>558.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>5.6431</td>\n",
              "      <td>341300.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>1627.0</td>\n",
              "      <td>280.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>259.0</td>\n",
              "      <td>3.8462</td>\n",
              "      <td>342200.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.85</td>\n",
              "      <td>52.0</td>\n",
              "      <td>919.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>413.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>4.0368</td>\n",
              "      <td>269700.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.84</td>\n",
              "      <td>52.0</td>\n",
              "      <td>2535.0</td>\n",
              "      <td>489.0</td>\n",
              "      <td>1094.0</td>\n",
              "      <td>514.0</td>\n",
              "      <td>3.6591</td>\n",
              "      <td>299200.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.84</td>\n",
              "      <td>52.0</td>\n",
              "      <td>3104.0</td>\n",
              "      <td>687.0</td>\n",
              "      <td>1157.0</td>\n",
              "      <td>647.0</td>\n",
              "      <td>3.1200</td>\n",
              "      <td>241400.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-122.26</td>\n",
              "      <td>37.84</td>\n",
              "      <td>42.0</td>\n",
              "      <td>2555.0</td>\n",
              "      <td>665.0</td>\n",
              "      <td>1206.0</td>\n",
              "      <td>595.0</td>\n",
              "      <td>2.0804</td>\n",
              "      <td>226700.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-122.25</td>\n",
              "      <td>37.84</td>\n",
              "      <td>52.0</td>\n",
              "      <td>3549.0</td>\n",
              "      <td>707.0</td>\n",
              "      <td>1551.0</td>\n",
              "      <td>714.0</td>\n",
              "      <td>3.6912</td>\n",
              "      <td>261100.0</td>\n",
              "      <td>NEAR BAY</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_house_value  ocean_proximity\n",
              "0    -122.23     37.88  ...            452600.0         NEAR BAY\n",
              "1    -122.22     37.86  ...            358500.0         NEAR BAY\n",
              "2    -122.24     37.85  ...            352100.0         NEAR BAY\n",
              "3    -122.25     37.85  ...            341300.0         NEAR BAY\n",
              "4    -122.25     37.85  ...            342200.0         NEAR BAY\n",
              "5    -122.25     37.85  ...            269700.0         NEAR BAY\n",
              "6    -122.25     37.84  ...            299200.0         NEAR BAY\n",
              "7    -122.25     37.84  ...            241400.0         NEAR BAY\n",
              "8    -122.26     37.84  ...            226700.0         NEAR BAY\n",
              "9    -122.25     37.84  ...            261100.0         NEAR BAY\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xDA2egjLx78",
        "colab_type": "text"
      },
      "source": [
        "Train/test split ratio of 70:30 using sklearn.model selection library with random state set to 2003."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh5Ze7lv6CA0",
        "colab_type": "code",
        "outputId": "2f281bb4-cdfe-4a15-cb37-b4e38704d77d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# predict the median_house_value column\n",
        "y=dataset['median_house_value']\n",
        "\n",
        "#The remainder of the columns will be used to predict Y\n",
        "#Select from the \" column to the median_income column\n",
        "x=dataset.loc[:,'longitude':'median_income']\n",
        "\n",
        "#Test size 0.3 means test sample will be 30% and training will be 70%. \n",
        "#Thus, ratio of train:test is 70:30\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=2003)\n",
        "\n",
        "# Converts the datasets to numpy arrays to work with our PyTorch model\n",
        "x_train_np=x_train.to_numpy()\n",
        "y_train_np=y_train.to_numpy()\n",
        "\n",
        "# Convert the testing data\n",
        "x_test_np=x_test.to_numpy()\n",
        "y_test_np=y_test.to_numpy()\n",
        "\n",
        "#Size of Trainable parameters can be find by checking the shape of training data.\n",
        "x_train_np.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14303, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2vausamRHm-",
        "colab_type": "text"
      },
      "source": [
        "We use here different activation the Sigmoid Function curve looks like an S-shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY0aa6ujbqwp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "creating the class MUST be a subclass of torch.nn.Module . the networks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEZWx-ZN6PuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the class MUST be a subclass of torch.nn.Module\n",
        "class cnnRegressor(torch.nn.Module):\n",
        "  # Define the initialization method\n",
        "  def __init__(self,batch_size,inputs,outputs):\n",
        "\n",
        "# Initialize the superclass and store the parameters\n",
        "    super(cnnRegressor, self).__init__()\n",
        "    self.batch_size=batch_size\n",
        "    self.inputs=inputs\n",
        "    self.outputs=outputs\n",
        "    # Define the input layer (input channels, output channels, kernel size)\n",
        "    self.input_layer=Conv1d(inputs, batch_size, 1)\n",
        "    # Define a max pooling layer (kernel size)\n",
        "    self.max_pooling_layer=MaxPool1d(1)\n",
        "    # Define another convolution layer\n",
        "    self.conv_layer=Conv1d(batch_size,128,1)\n",
        "    # Define a flatten layer\n",
        "    self.flatten_layer=Flatten()\n",
        "    # Define a linear layer ( outputs)\n",
        "    self.linear_layer=Linear(128,64)\n",
        "    # Finally, define the output layer\n",
        "    self.output_layer=Linear(64,outputs)\n",
        "    # Define a method to feed inputs through the model\n",
        "  def feed(self, input):\n",
        "    # Reshape the entry so it can be fed to the input layer\n",
        "    # Although we’re using 1D convolution, it still expects a 3D array to process in a 1D fashion\n",
        "     input = input.reshape(self.batch_size, self.inputs, 1)\n",
        "\n",
        "  # Get the output of the first layer and run it through the the Sigmoid activation function.\n",
        "     output = sigmoid(self.input_layer(input))\n",
        "\n",
        "     # Get the output of the max pooling layer\n",
        "     output = sigmoid(self.max_pooling_layer(output))\n",
        "                                      \n",
        "   # Get the output of the second convolution layer and run it through the Sigmoid activation function\n",
        "     output = sigmoid(self.conv_layer(output))\n",
        "\n",
        "     # Get the output of the flatten layer\n",
        "     output = self.flatten_layer(output)\n",
        "     # Get the output of the linear layer and run it through the Sigmoid activation function\n",
        "     output = self.linear_layer(output)\n",
        "     # Finally, get the output of the output layer and return it\n",
        "     output = self.output_layer(output)\n",
        "     return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Th_7y_7B5m",
        "colab_type": "code",
        "outputId": "86195107-dee7-4d52-adbd-b16aac04877e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Import the Adam (stochastic gradient descent) package from pytorch for\n",
        "# our optimizer\n",
        "from torch.optim import Adam\n",
        "# Import the L1Loss (mean absolute error loss) package from pytorch for\n",
        "# our performance measure\n",
        "from torch.nn import L1Loss\n",
        "# Import the R^2 score package from pytorch's ignite for our score measure\n",
        "# This package is not installed by default so the next line does that\n",
        "!pip install pytorch-ignite\n",
        "from ignite.contrib.metrics.regression.r2_score import R2Score\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3WaNKJQMNNA",
        "colab_type": "text"
      },
      "source": [
        "The kernel_size is 1X1. Due to this convolution operation is also known as 1-d convolution.  Define the  CNN model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWvzkgkH6sa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the batch size we'd like to use\n",
        "batch_size = 64\n",
        "#(batch size, X columns, Y columns)\n",
        "model =cnnRegressor(batch_size, x.shape[1], 1)\n",
        "# Set the model to use the GPU for processing\n",
        "model.cuda()\n",
        "\n",
        "# saving the model\n",
        "torch.save(model.state_dict(), \"student_id:(0873122)1dconv reg.model\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DCdO3tTQZV2",
        "colab_type": "text"
      },
      "source": [
        " Number of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwf52BP279iF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sum(p.numel() for p in model.parameters()))\n",
        "#pytorch_total_params = sum(p.numel() for p in model.parameters())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRRiQlMltrX8",
        "colab_type": "text"
      },
      "source": [
        "Create a method for running the batches of data through the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA7euI377tTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This method will return the average L1 loss and R^2 score of the passed model on the passed DataLoader\n",
        "def model_loss(model, dataset, train = False, optimizer = None):\n",
        "\n",
        "# Cycle through the batches and get the average L1 loss\n",
        "  performance = L1Loss()\n",
        "  score_metric = R2Score()\n",
        "\n",
        "  avg_loss = 0\n",
        "\n",
        "  avg_score = 0\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  for input, output in iter(dataset):\n",
        "\n",
        "# Get the model's predictions for the training dataset\n",
        "    predictions = model.feed(input)\n",
        "\n",
        "# Get the model's loss\n",
        "    loss = performance(predictions, output)\n",
        "\n",
        "# Get the model's R^2 score\n",
        "    score_metric.update([predictions, output])\n",
        "    score = score_metric.compute()\n",
        "\n",
        "    if(train):\n",
        "\n",
        "# Clear any errors so they don't cummulate\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "# Compute the gradients for our optimizer\n",
        "      loss.backward()\n",
        "\n",
        "# Use the optimizer to update the model's parameters based on the gradients\n",
        "      optimizer.step()\n",
        "\n",
        "# Store the loss and update the counter\n",
        "    avg_loss += loss.item()\n",
        "    avg_score += score\n",
        "    count += 1\n",
        "\n",
        "  return avg_loss / count, avg_score/ count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxxtrOE1R674",
        "colab_type": "text"
      },
      "source": [
        "Here we increase the size of the epochs and change the optimizer and the Learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "776xKuHx7z_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the number of epochs to train for \n",
        "\n",
        "epochs = 100\n",
        "\n",
        "# Define the performance measure and optimizer Import the Adam (stochastic gradient descent) package from pytorch for\n",
        "# our optimizer\n",
        "optimizer = Adam(model.parameters(), lr = 1e-3)\n",
        "\n",
        "# Convert the training set into torch variables for our model using the GPU\n",
        "# as floats. The reshape is to remove a warning pytorch outputs otherwise.\n",
        "inputs = torch.from_numpy(x_train_np).cuda().float()\n",
        "outputs = torch.from_numpy(y_train_np.reshape(y_train_np.shape[0], 1)).cuda().float()\n",
        "\n",
        "# Create a DataLoader instance to work with our batches\n",
        "tensor = TensorDataset(inputs, outputs)\n",
        "loader = DataLoader(tensor, batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "start_time = time.time()\n",
        "# Start the training loop\n",
        "for epoch in range(epochs):\n",
        "  # Cycle through the batches and get the average loss\n",
        "  avg_loss, avg_r2_score = model_loss (model, loader, train = True, optimizer = optimizer)\n",
        "\n",
        "# Output the average loss\n",
        "print(\"Epoch \" + str(epoch + 1) + \":\\n\\tLoss = \" + str(avg_loss) + \"\\n\\tR^2 Score = \" + str(avg_r2_score))\n",
        "\n",
        " #end_time= time.time()\n",
        "\n",
        "model = model.eval()\n",
        "print('training time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "print('training time elapsed: %.2f min' % ((time.time() - start_time)/60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQUOAB6vwBKa",
        "colab_type": "text"
      },
      "source": [
        "Finally, we will test the model to see how it performs on the testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuJLO7dP8Qj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the testing set into torch variables for our model using the GPU as floats\n",
        "inputs = torch.from_numpy(x_test_np).cuda().float()\n",
        "outputs = torch.from_numpy(y_test_np.reshape(y_test_np.shape[0], 1)).cuda().float()\n",
        "\n",
        "# Create a DataLoader instance to work with our batches\n",
        "tensor = TensorDataset(inputs, outputs)\n",
        "loader = DataLoader(tensor, batch_size, shuffle = True, drop_last=True)\n",
        "\n",
        "# Output the average performance of the model\n",
        "avg_loss, avg_r2_score = model_loss(model, loader)\n",
        "print(\"The model's L1 loss is: \" + str(avg_loss))\n",
        "print(\"The model's R^2 score is: \" + str(avg_r2_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34-VLI1YXXs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz_TDej7S1hp",
        "colab_type": "text"
      },
      "source": [
        "Figure 1: Example sub-plots of first eighteen samples from the dataset in a single figure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2OqbIFz8hlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = dataset.columns.drop(['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income'])\n",
        "\n",
        "x= range(0, dataset.shape[0])\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "for column in dataset:\n",
        "  ax.plot(x, dataset[column],label=column)\n",
        "\n",
        "dataset.plot(subplots=True)\n",
        "\n",
        "ax.set_title('housing Dataset')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kG10amWd3qL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start.record()\n",
        "z = x + y\n",
        "end.record()\n",
        "\n",
        "# Waits for everything to finish running\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "print(start.elapsed_time(end))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}